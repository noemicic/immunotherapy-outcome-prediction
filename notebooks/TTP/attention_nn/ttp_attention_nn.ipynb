{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8de441",
   "metadata": {},
   "source": [
    "# TTP: advanced NN architectures (attention-based ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae59d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#Import sklearn libraries for data preprocessing\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Import keras libraries for building the models\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "#Import libraries for model evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "#Import SHAP library for model interpretability\n",
    "import shap\n",
    "\n",
    "# for reproducibility, the value is set for conventional reasons\n",
    "SEED = 42\n",
    "tf.keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa9bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = pd.read_csv('datased_d.csv', encoding='latin-1', sep=',') # request the dataset to the author\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target column : \"progression\", regression task\n",
    "# relevant columns for the model\n",
    "relevant_columns = [ 'age', 'sex', 'smoking', 'ps_at_diagnosis_ad', 'n#_mets_sites', 'lung_only_m1', 'pleural', 'pericard', 'lymph_nodes_only_m1', 'soft_tissue',\n",
    "    'leptomingeal','skin','peritoneal','renal','pancreas', 'brain', 'liver', 'bone', 'adrenal', 'histology', 'hbbaselineio','leucotbaselineio',\n",
    "    'neut_abs...143','linfo_abs...144','baso_abs...145', 'mono_abs...147', 'plaqtbaselineio', 'progression']\n",
    "\n",
    "df = data[relevant_columns]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bbf123",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to randomize the data\n",
    "df = df.sample(frac=1, random_state=SEED)\n",
    "\n",
    "var_int = ['ps_at_diagnosis_ad', 'n#_mets_sites', 'lung_only_m1', 'pleural', 'pericard', 'lymph_nodes_only_m1', 'soft_tissue',\n",
    "    'leptomingeal','skin','peritoneal','renal','pancreas', 'brain', 'liver', 'bone', 'adrenal']\n",
    "for i in var_int:\n",
    "    df[i] = df[i].astype(int)\n",
    "    \n",
    "df['sex'] = df['sex'].str.lower()\n",
    "\n",
    "# one-hot encoding\n",
    "one_hot_data = pd.get_dummies(df, columns=['histology', 'sex', 'smoking'])\n",
    "\n",
    "cols_to_convert = ['histology_adenocarcinoma','histology_nsclc', 'histology_squamous', 'sex_female', 'sex_male',\n",
    "                   'smoking_current', 'smoking_former', 'smoking_non-smoker']\n",
    "\n",
    "one_hot_data[cols_to_convert] = one_hot_data[cols_to_convert].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features and target\n",
    "X = one_hot_data[one_hot_data.columns.difference(['progression'])] \n",
    "y = df['progression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ccb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training, validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a1deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_features = ['lung_only_m1', 'pleural', 'pericard', 'lymph_nodes_only_m1', 'soft_tissue', 'leptomingeal','skin','peritoneal','renal',\n",
    "                   'pancreas', 'brain', 'liver', 'bone', 'adrenal','histology_adenocarcinoma', 'histology_nsclc', \n",
    "                   'histology_squamous', 'sex_female', 'sex_male','smoking_current', 'smoking_former', 'smoking_non-smoker']\n",
    "numeric_features = ['hbbaselineio','leucotbaselineio', 'neut_abs...143','linfo_abs...144','baso_abs...145', 'mono_abs...147',\n",
    "                    'plaqtbaselineio', 'age', 'ps_at_diagnosis_ad', 'n#_mets_sites', ]\n",
    "\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled = X_val.copy() \n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_val_scaled = X_train_val.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_val_scaled[numeric_features] = scaler.fit_transform(X_train_val_scaled[numeric_features])\n",
    "X_train_scaled[numeric_features] = scaler.fit_transform(X_train_scaled[numeric_features])\n",
    "X_val_scaled[numeric_features] = scaler.transform(X_val_scaled[numeric_features])\n",
    "X_test_scaled[numeric_features] = scaler.transform(X_test_scaled[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571cf6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_f(y_true, y_pred):\n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    denominator = tf.maximum(\n",
    "        (tf.abs(y_true) + tf.abs(y_pred) + epsilon) / 2.0,\n",
    "        epsilon\n",
    "    )\n",
    "    diff = tf.abs(y_true - y_pred)\n",
    "    return 100 * tf.reduce_mean(diff / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff08150",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Layer definition\n",
    "class FeatureAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FeatureAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Net to compute the attention weights\n",
    "        self.attention_dense = layers.Dense(\n",
    "            input_shape[-1], \n",
    "            activation='tanh',\n",
    "            name='attention_dense'\n",
    "        )\n",
    "        self.attention_output = layers.Dense(\n",
    "            input_shape[-1], \n",
    "            activation='softmax',\n",
    "            name='attention_output'\n",
    "        )\n",
    "        super(FeatureAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Compute the attention weights of each sample\n",
    "        attention_weights = self.attention_dense(inputs)\n",
    "        attention_weights = self.attention_output(attention_weights)\n",
    "        \n",
    "        # Apply the attention through element-wise operation\n",
    "        attended_features = inputs * attention_weights\n",
    "        \n",
    "        return attended_features\n",
    "\n",
    "\n",
    "# 1. Attention-block at input level\n",
    "def model_attention_input_only(input_shape):\n",
    "    return models.Sequential([\n",
    "        FeatureAttention(input_shape=input_shape),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "# 2. Attention-block at intermediate level\n",
    "def model_attention_intermediate(input_shape):\n",
    "    return models.Sequential([\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=input_shape),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        FeatureAttention(),  \n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "# 3. Multiple attention blocks\n",
    "def model_multiple_attention(input_shape):\n",
    "    return models.Sequential([\n",
    "        FeatureAttention(input_shape=input_shape),  \n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        FeatureAttention(),  \n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "# 4. Attention block right before the output\n",
    "def model_attention_pre_output(input_shape):\n",
    "    return models.Sequential([\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=input_shape),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.4),\n",
    "        FeatureAttention(),  \n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "# 5. Residual attention\n",
    "class ResidualAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ResidualAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attention = FeatureAttention()\n",
    "        super(ResidualAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attended = self.attention(inputs)\n",
    "        return inputs + attended  \n",
    "\n",
    "def model_residual_attention(input_shape):\n",
    "    return models.Sequential([\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=input_shape),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        ResidualAttention(),  \n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# Esempio di utilizzo e confronto\n",
    "def compare_models(X_train, y_train, X_val, y_val):\n",
    "    input_shape = (X_train.shape[1],)\n",
    "    \n",
    "    models_to_test = {\n",
    "        'attention_input': model_attention_input_only(input_shape),\n",
    "        'attention_intermediate': model_attention_intermediate(input_shape),\n",
    "        'attention_pre_output': model_attention_pre_output(input_shape),\n",
    "        'multiple_attention': model_multiple_attention(input_shape),\n",
    "        'residual_attention': model_residual_attention(input_shape)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models_to_test.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss='mae',  \n",
    "                  metrics=['mae', 'mse', smape_f])\n",
    "        \n",
    "        history = model.fit(X_train, y_train,\n",
    "                         validation_data=(X_val, y_val),\n",
    "                         epochs=500,\n",
    "                         batch_size=16,\n",
    "                         verbose=0\n",
    "        )\n",
    "\n",
    "        epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Plot of the Loss \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "        plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot of MAE \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, history.history['mae'], 'b', label='Training MAE')\n",
    "        plt.plot(epochs, history.history['val_mae'], 'r', label='Validation MAE')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.title('Training and Validation MAE')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        y_pred = model.predict(X_test_scaled).flatten()  \n",
    "        \n",
    "        # Main metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100  # 1e-8 to avoid the division for zero\n",
    "        smape = 100/len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred) + 1e-8))\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "        print(f\"SMAPE: {smape:.2f}%\")\n",
    "\n",
    "        results = pd.DataFrame({\n",
    "            'ttp_true': y_test,       # true values\n",
    "            'ttpduree_pred': y_pred        # predicted values\n",
    "        })\n",
    "\n",
    "        print(results.tail(10))\n",
    "        \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  \n",
    "        plt.xlabel(\"True Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.title(\"Predicted vs True Values\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "        explainer = shap.Explainer(model, X_train_scaled_df)  \n",
    "        \n",
    "        shap_values = explainer(X_test_scaled_df)\n",
    "\n",
    "        print(\"SHAP summary plot:\")\n",
    "        shap.plots.beeswarm(\n",
    "            shap_values,\n",
    "            max_display=len(binary_features + numeric_features)\n",
    "        )\n",
    "\n",
    "                \n",
    "            \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_models(X_train_scaled, y_train, X_val_scaled, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
