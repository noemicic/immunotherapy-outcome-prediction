{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d1c1a43",
   "metadata": {},
   "source": [
    "# OS Duration: advanced NN architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6bed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#Importing to preprocess the data\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Importing to build the models\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "#Importing to evaluate the models\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "#Importing to explain the models\n",
    "import shap\n",
    "\n",
    "# for reproducibility, the value is set for conventional reasons\n",
    "SEED = 42\n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2206ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = pd.read_csv('dataset_b.', encoding='latin-1', sep=',') # request the dataset to the author\n",
    "\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target column : \"osduree\", continuous variable\n",
    "# relevant columns\n",
    "relevant_columns = ['age', 'dcr', 'dnlr', 'histology', 'immuno_line', 'iorr', \n",
    "                    'ldhpre', 'leucotpre', 'nb_meta_beforeimmuno', 'neuttpre', \n",
    "                     'ps_befimmuno', 'sex', 'smoking_history', 'osduree']\n",
    "\n",
    "data = data[relevant_columns]\n",
    "data = data.dropna(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"encode\" the categorical variables\n",
    "data['histology'] = data['histology'].str.lower()\n",
    "data['sex'] = data['sex'].str.lower()\n",
    "data['smoking_history'] = data['smoking_history'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d22b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to randomize the data\n",
    "data = data.sample(frac=1, random_state=SEED)\n",
    "\n",
    "# one-hot encoding\n",
    "one_hot_data = pd.get_dummies(data, columns=['histology', 'sex', 'smoking_history'])\n",
    "\n",
    "one_hot_data = one_hot_data.rename(columns={\n",
    "    'histology_Adenocarcinoma': 'histology_adenocarcinoma',\n",
    "    'histology_Squamous': 'histology_squamous',\n",
    "    'histology_Nsclc_other': 'histology_nsclc_other',\n",
    "    'histology_Large_cells': 'histology_large_cells',\n",
    "    'sex_Male': 'sex_male',\n",
    "    'sex_Female': 'sex_female',\n",
    "    'smoking_history_Non_smoker': 'smoking_history_non_smoker',\n",
    "    'smoking_history_Former': 'smoking_history_former',\n",
    "    'smoking_history_Current': 'smoking_history_current',\n",
    "    'smoking_history_Unk': 'smoking_history_unk'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f789ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace boolean values with 0 and 1\n",
    "for col in ['histology_adenocarcinoma','histology_squamous','histology_nsclc other',\n",
    "    'histology_large cells','sex_male','sex_female','smoking_history_non smoker','smoking_history_former','smoking_history_current',\n",
    "     'smoking_history_unk']:\n",
    "    one_hot_data[col] = one_hot_data[col].replace({False: 0, True: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features and target\n",
    "X = one_hot_data[one_hot_data.columns.difference(['osduree'])]\n",
    "y = data['osduree']\n",
    "\n",
    "\n",
    "# First division: training+validation vs test (80% vs 20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42  \n",
    ")\n",
    "\n",
    "# Second division: training vs validation (75% vs 25% of 80%)\n",
    "# This means 60% training, 20% validation, 20% test\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ded4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This ensures that all numerical features contribute equally\n",
    "numerical_features = ['age', 'dcr', 'dnlr', 'ldhpre', 'leucotpre', \n",
    "                      'nb_meta_beforeimmuno', 'neuttpre', 'ps_befimmuno']\n",
    "\n",
    "binary_features = [col for col in X.columns if col not in numerical_features]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled = X_val.copy() \n",
    "X_test_scaled = X_test.copy()\n",
    "X_scaled = X.copy()\n",
    "X_train_val_scaled = X_temp.copy()\n",
    "\n",
    "X_scaled[numerical_features] = scaler.fit_transform(X_scaled[numerical_features])\n",
    "X_train_scaled[numerical_features] = scaler.fit_transform(X_train_scaled[numerical_features])\n",
    "X_val_scaled[numerical_features] = scaler.transform(X_val_scaled[numerical_features])\n",
    "X_test_scaled[numerical_features] = scaler.transform(X_test_scaled[numerical_features])\n",
    "X_train_val_scaled[numerical_features] = scaler.fit_transform(X_train_val_scaled[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ef9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_f(y_true, y_pred):\n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    denominator = tf.maximum(\n",
    "        (tf.abs(y_true) + tf.abs(y_pred) + epsilon) / 2.0,\n",
    "        epsilon\n",
    "    )\n",
    "    diff = tf.abs(y_true - y_pred)\n",
    "    return 100 * tf.reduce_mean(diff / denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568444a2",
   "metadata": {},
   "source": [
    "## Funnel MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b942425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Funnel MLP (Deep Funnel Network)\n",
    "#    Input → 512 → BN → ReLU → DO(0.3)\n",
    "#          → 256 → BN → ReLU → DO(0.3)\n",
    "#          → 128 → BN → ReLU → DO(0.2)\n",
    "#          →  64 → BN → ReLU → DO(0.2)\n",
    "#          → Output\n",
    "\n",
    "mlp_model = tf.keras.Sequential([\n",
    "        layers.Dense(512, activation='relu', input_shape=(X.shape[1],)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "mlp_model.compile(optimizer='adam',\n",
    "                  loss='mae',  \n",
    "                  metrics=['mae', 'mse', smape_f])\n",
    "\n",
    "# Train the model\n",
    "history = mlp_model.fit(X_train_scaled, y_train,\n",
    "                         validation_data=(X_val_scaled, y_val),\n",
    "                         epochs=500, batch_size=16)\n",
    "\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot of the Loss \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot of MAE \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history.history['mae'], 'b', label='Training MAE')\n",
    "plt.plot(epochs, history.history['val_mae'], 'r', label='Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbde55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = mlp_model.predict(X_test_scaled).flatten()  \n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100  # 1e-8 to avoid division by zero\n",
    "smape = 100/len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred) + 1e-8))\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"SMAPE: {smape:.2f}%\")\n",
    "\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'osduree_true': y_test,       # real values\n",
    "    'osduree_pred': y_pred        # predicted values\n",
    "})\n",
    "\n",
    "print(results.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deedce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  \n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs True Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9311c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(mlp_model, X_train_scaled_df) \n",
    "\n",
    "shap_values = explainer(X_test_scaled_df)\n",
    "\n",
    "print(\"SHAP summary plot:\")\n",
    "shap.plots.beeswarm(\n",
    "    shap_values,\n",
    "    max_display=len(binary_features + numerical_features)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1273eb8",
   "metadata": {},
   "source": [
    "## WideRes MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. WideRes MLP, Wide‑but‑Shallow Residual MLP\n",
    "#    Input → 1024 → ReLU → DO(0.3) → add(input→dense(1024))\n",
    "#          → 1024 → ReLU → DO(0.3)\n",
    "#          → 1024 → ReLU → DO(0.3)\n",
    "#          → Output\n",
    "\n",
    "def build_model_residual_wide(input_dim):\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(1024, activation='relu')(inp)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    # Residual branch\n",
    "    res = layers.Dense(1024)(inp)\n",
    "    x = layers.Add()([x, res])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(1, activation='linear')(x)\n",
    "    return tf.keras.Model(inp, out)\n",
    "\n",
    "mlp_model = build_model_residual_wide(X.shape[1])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "mlp_model.compile(optimizer='adam',\n",
    "                  loss='mae',  \n",
    "                  metrics=['mae', 'mse', smape_f])\n",
    "\n",
    "# Train the model\n",
    "history = mlp_model.fit(X_train_scaled, y_train,\n",
    "                         validation_data=(X_val_scaled, y_val),\n",
    "                         epochs=500, batch_size=16)\n",
    "\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot of the Loss \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot of MAE \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history.history['mae'], 'b', label='Training MAE')\n",
    "plt.plot(epochs, history.history['val_mae'], 'r', label='Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d0332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = mlp_model.predict(X_test_scaled).flatten()  \n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100  # 1e-8 to avoid division by zero\n",
    "smape = 100/len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred) + 1e-8))\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"SMAPE: {smape:.2f}%\")\n",
    "\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'osduree_true': y_test,       # real values\n",
    "    'osduree_pred': y_pred        # predicted values\n",
    "})\n",
    "\n",
    "print(results.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95fa943",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # linea ideale y=x\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs True Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b71d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(mlp_model, X_train_scaled_df) \n",
    "\n",
    "shap_values = explainer(X_test_scaled_df)\n",
    "\n",
    "print(\"SHAP summary plot:\")\n",
    "shap.plots.beeswarm(\n",
    "    shap_values,\n",
    "    max_display=len(binary_features + numerical_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2f645b",
   "metadata": {},
   "source": [
    "## Self‑Norm MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b621828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Self‑Norm MLP, Self‑Normalizing Network with SELU & AlphaDropout\n",
    "#    Input → 512 → SELU → AD(0.1) → add(input→dense(512))\n",
    "#          → 512 → SELU → AD(0.1) → add(input→dense(512))       \n",
    "#          → 256 → SELU → AD(0.1) → add(input→dense(256))\n",
    "#          → Output\n",
    "\n",
    "\n",
    "mlp_model = tf.keras.Sequential([\n",
    "        layers.Dense(512, activation='selu', input_shape=(X.shape[1],)),\n",
    "        layers.AlphaDropout(0.1),\n",
    "        layers.Dense(512, activation='selu'),\n",
    "        layers.AlphaDropout(0.1),\n",
    "        layers.Dense(256, activation='selu'),\n",
    "        layers.AlphaDropout(0.1),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "mlp_model.compile(optimizer='adam',\n",
    "                  loss='mae',  \n",
    "                  metrics=['mae', 'mse', smape_f])\n",
    "\n",
    "# Train the model\n",
    "history = mlp_model.fit(X_train_scaled, y_train,\n",
    "                         validation_data=(X_val_scaled, y_val),\n",
    "                         epochs=500, batch_size=16)\n",
    "\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot of the Loss \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot of MAE \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history.history['mae'], 'b', label='Training MAE')\n",
    "plt.plot(epochs, history.history['val_mae'], 'r', label='Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd12fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = mlp_model.predict(X_test_scaled).flatten()  \n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100  # 1e-8 to avoid division by zero\n",
    "smape = 100/len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred) + 1e-8))\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"SMAPE: {smape:.2f}%\")\n",
    "\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'osduree_true': y_test,       # real values\n",
    "    'osduree_pred': y_pred        # predicted values\n",
    "})\n",
    "\n",
    "print(results.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d391a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # linea ideale y=x\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs True Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(mlp_model, X_train_scaled_df) \n",
    "\n",
    "shap_values = explainer(X_test_scaled_df)\n",
    "\n",
    "print(\"SHAP summary plot:\")\n",
    "shap.plots.beeswarm(\n",
    "    shap_values,\n",
    "    max_display=len(binary_features + numerical_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e48eb5",
   "metadata": {},
   "source": [
    "## Stacked Narrow MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d621ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Stacked Narrow MLP, Deep Narrow‑and‑Deep MLP\n",
    "#    Input → 256 → LeakyReLU → DO(0.25) → add(input→dense(256))\n",
    "#          → 128 → LeakyReLU → DO(0.25) → add(input→dense(128))\n",
    "#          → 128 → LeakyReLU → DO(0.2)  → add(input→dense(128))\n",
    "#          → 64  → LeakyReLU → DO(0.2)  → add(input→dense(64))\n",
    "#          → 32  → LeakyReLU → DO(0.2)  → add(input→dense(32))\n",
    "#          → 16  → LeakyReLU → DO(0.15) → add(input→dense(16))\n",
    "#          → Output\n",
    "\n",
    "\n",
    "mlp_model = tf.keras.Sequential([\n",
    "        layers.Dense(256, input_shape=(X.shape[1],)),\n",
    "        LeakyReLU(),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Dense(128),\n",
    "        LeakyReLU(),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Dense(128),\n",
    "        LeakyReLU(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64),\n",
    "        LeakyReLU(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32),\n",
    "        LeakyReLU(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(16),\n",
    "        LeakyReLU(),\n",
    "        layers.Dropout(0.15),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "mlp_model.compile(optimizer='adam',\n",
    "                  loss='mae',  \n",
    "                  metrics=['mae', 'mse', smape_f])\n",
    "\n",
    "# Train the model\n",
    "history = mlp_model.fit(X_train_scaled, y_train,\n",
    "                         validation_data=(X_val_scaled, y_val),\n",
    "                         epochs=500, batch_size=16)\n",
    "\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot of the Loss \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot of MAE \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history.history['mae'], 'b', label='Training MAE')\n",
    "plt.plot(epochs, history.history['val_mae'], 'r', label='Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = mlp_model.predict(X_test_scaled).flatten()  \n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100  # 1e-8 to avoid division by zero\n",
    "smape = 100/len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred) + 1e-8))\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"SMAPE: {smape:.2f}%\")\n",
    "\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'osduree_true': y_test,       # real values\n",
    "    'osduree_pred': y_pred        # predicted values\n",
    "})\n",
    "\n",
    "print(results.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba85bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # linea ideale y=x\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs True Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e78c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(mlp_model, X_train_scaled_df) \n",
    "\n",
    "shap_values = explainer(X_test_scaled_df)\n",
    "\n",
    "print(\"SHAP summary plot:\")\n",
    "shap.plots.beeswarm(\n",
    "    shap_values,\n",
    "    max_display=len(binary_features + numerical_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87aa721",
   "metadata": {},
   "source": [
    "## NoisyWide MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab1183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. NoisyWide MLP, Gaussian Noise‑Augmented Wide MLP\n",
    "\n",
    "mlp_model = tf.keras.Sequential([\n",
    "        layers.GaussianNoise(0.1, input_shape=(X.shape[1],)),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "mlp_model.compile(optimizer='adam',\n",
    "                  loss='mae',  \n",
    "                  metrics=['mae', 'mse', smape_f])\n",
    "\n",
    "# Train the model\n",
    "history = mlp_model.fit(X_train_scaled, y_train,\n",
    "                         validation_data=(X_val_scaled, y_val),\n",
    "                         epochs=500, batch_size=16)\n",
    "\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot of the Loss \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot of MAE \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history.history['mae'], 'b', label='Training MAE')\n",
    "plt.plot(epochs, history.history['val_mae'], 'r', label='Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deee330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = mlp_model.predict(X_test_scaled).flatten()  \n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100  # 1e-8 to avoid division by zero\n",
    "smape = 100/len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred) + 1e-8))\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"SMAPE: {smape:.2f}%\")\n",
    "\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'osduree_true': y_test,       # real values\n",
    "    'osduree_pred': y_pred        # predicted values\n",
    "})\n",
    "\n",
    "print(results.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # linea ideale y=x\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs True Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(mlp_model, X_train_scaled_df) \n",
    "\n",
    "shap_values = explainer(X_test_scaled_df)\n",
    "\n",
    "print(\"SHAP summary plot:\")\n",
    "shap.plots.beeswarm(\n",
    "    shap_values,\n",
    "    max_display=len(binary_features + numerical_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a13ece",
   "metadata": {},
   "source": [
    "## ParallelBranch MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c0f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. ParallelBranch MLP, Multi‑Branch Ensemble MLP\n",
    "# Input\n",
    "# ├─ Branch A: Dense(512)→ReLU→Dropout(0.3)→Dense(256)→ReLU\n",
    "# └─ Branch B: Dense(512)→ReLU→Dropout(0.3)→Dense(256)→ReLU\n",
    "# Concat → Dense(128)→ReLU→Dropout(0.2) → Output\n",
    "\n",
    "def build_model_multibranch(input_dim):\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    # Branch A\n",
    "    a = layers.Dense(512, activation='relu')(inp)\n",
    "    a = layers.Dropout(0.3)(a)\n",
    "    a = layers.Dense(256, activation='relu')(a)\n",
    "    # Branch B\n",
    "    b = layers.Dense(512, activation='relu')(inp)\n",
    "    b = layers.Dropout(0.3)(b)\n",
    "    b = layers.Dense(256, activation='relu')(b)\n",
    "    # Merge\n",
    "    x = layers.Concatenate()([a, b])\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(1, activation='linear')(x)\n",
    "    return tf.keras.Model(inp, out)\n",
    "\n",
    "mlp_model = build_model_multibranch(X.shape[1])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "mlp_model.compile(optimizer='adam',\n",
    "                  loss='mae',  \n",
    "                  metrics=['mae', 'mse', smape_f])\n",
    "\n",
    "# Train the model\n",
    "history = mlp_model.fit(X_train_scaled, y_train,\n",
    "                         validation_data=(X_val_scaled, y_val),\n",
    "                         epochs=500, batch_size=16)\n",
    "\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot of the Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot of MAE \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history.history['mae'], 'b', label='Training MAE')\n",
    "plt.plot(epochs, history.history['val_mae'], 'r', label='Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = mlp_model.predict(X_test_scaled).flatten()  \n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100  # 1e-8 to avoid division by zero\n",
    "smape = 100/len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred) + 1e-8))\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"SMAPE: {smape:.2f}%\")\n",
    "\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'osduree_true': y_test,       # real values\n",
    "    'osduree_pred': y_pred        # predicted values\n",
    "})\n",
    "\n",
    "print(results.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786aeac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # linea ideale y=x\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs True Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf520e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(mlp_model, X_train_scaled_df) \n",
    "\n",
    "shap_values = explainer(X_test_scaled_df)\n",
    "\n",
    "print(\"SHAP summary plot:\")\n",
    "shap.plots.beeswarm(\n",
    "    shap_values,\n",
    "    max_display=len(binary_features + numerical_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b626545",
   "metadata": {},
   "source": [
    "## L2Light MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d59988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. L2Light MLP, Lightweight L2‑Heavy Regularized MLP\n",
    "\n",
    "mlp_model = tf.keras.Sequential([\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(X.shape[1],)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.005)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "mlp_model.compile(optimizer='adam',\n",
    "                  loss='mae',  \n",
    "                  metrics=['mae', 'mse', smape_f])\n",
    "\n",
    "# Train the model\n",
    "history = mlp_model.fit(X_train_scaled, y_train,\n",
    "                         validation_data=(X_val_scaled, y_val),\n",
    "                         epochs=500, batch_size=16)\n",
    "\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot of the Loss \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot of MAE \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history.history['mae'], 'b', label='Training MAE')\n",
    "plt.plot(epochs, history.history['val_mae'], 'r', label='Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = mlp_model.predict(X_test_scaled).flatten()  \n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100  # 1e-8 to avoid division by zero\n",
    "smape = 100/len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred) + 1e-8))\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"SMAPE: {smape:.2f}%\")\n",
    "\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'osduree_true': y_test,       # real values\n",
    "    'osduree_pred': y_pred        # predicted values\n",
    "})\n",
    "\n",
    "print(results.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4bae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # linea ideale y=x\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs True Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(mlp_model, X_train_scaled_df) \n",
    "\n",
    "shap_values = explainer(X_test_scaled_df)\n",
    "\n",
    "print(\"SHAP summary plot:\")\n",
    "shap.plots.beeswarm(\n",
    "    shap_values,\n",
    "    max_display=len(binary_features + numerical_features)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
